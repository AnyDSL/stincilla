fn @set_pixeli16_fn(img: Img) = @|idx: i32, val: i16| bitcast[&mut addrspace(1)[i16], &[i8]](img.buf.data)(idx) = val
fn @get_pixeli16_fn(img: Img) = @|idx: i32| bitcast[&addrspace(1)[i16], &[i8]](img.buf.data)(idx)

fn @for_shift_down(body: fn(i32) -> ()) {
    fn @(?a & ?b) loop(a: i32, b: i32) -> () {
        if a > b {
            @body(a);
            loop(a >> 1, b);
        }
    }
    loop
}
fn @for_shift_up(body: fn(i32) -> ()) {
    fn @(?a & ?b) loop(a: i32, b: i32) -> () {
        if a < b {
            @body(a);
            loop(a << 1, b);
        }
    }
    loop
}

fn @iteration1f1i(body: fn(i32, i32, Acc, Acci16) -> ()) = @|out: Img, arr: Img, bh_lower: BoundaryFni16, bh_upper: BoundaryFni16| {
    let coarsening_factor = 1;
    let acc   = accelerator(device_id);
    let grid  = (out.width, out.height / coarsening_factor, 1);
    let block = (32, 4, 1);

    let out_gpu = alloc_img[f32](out, acc.alloc);
    let arr_gpu = alloc_img[i16](arr, acc.alloc);
    copy_img(arr, arr_gpu);

    for benchmark_acc(acc) {
        for work_item in acc.exec(grid, block) {
            let bdim_y = work_item.bdimy();
            let gid_x  = work_item.tidx() + work_item.bdimx() * work_item.bidx();
            let gid_y  = work_item.tidy() + work_item.bdimy() * work_item.bidy() * coarsening_factor;
            let out_acc = get_acc(out_gpu, set_pixel_fn(out_gpu), get_pixel_fn(out_gpu));
            let arr_acc = get_acci16_bh(arr_gpu, set_pixeli16_fn(arr_gpu), get_pixeli16_fn(arr_gpu), (Boundary::Unknown, Boundary::Unknown), bh_lower, bh_upper);

            for i in unroll(0, coarsening_factor) {
                @body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc);
            }
        }
    }

    copy_img(out_gpu, out);
    release(out_gpu.buf);
    release(arr_gpu.buf);
}
fn @iteration2i(body: fn(i32, i32, Acci16, Acci16) -> ()) = @|out: Img, arr: Img, bh_lower: BoundaryFni16, bh_upper: BoundaryFni16| {
    let coarsening_factor = 1;
    let acc   = accelerator(device_id);
    let grid  = (out.width, out.height / coarsening_factor, 1);
    let block = (32, 4, 1);

    let out_gpu = alloc_img[i16](out, acc.alloc);
    let arr_gpu = alloc_img[i16](arr, acc.alloc);
    copy_img(arr, arr_gpu);

    for benchmark_acc(acc) {
        for work_item in acc.exec(grid, block) {
            let bdim_y = work_item.bdimy();
            let gid_x  = work_item.tidx() + work_item.bdimx() * work_item.bidx();
            let gid_y  = work_item.tidy() + work_item.bdimy() * work_item.bidy() * coarsening_factor;
            let out_acc = get_acci16(out_gpu, set_pixeli16_fn(out_gpu), get_pixeli16_fn(out_gpu));
            let arr_acc = get_acci16_bh(arr_gpu, set_pixeli16_fn(arr_gpu), get_pixeli16_fn(arr_gpu), (Boundary::Unknown, Boundary::Unknown), bh_lower, bh_upper);

            for i in unroll(0, coarsening_factor) {
                @body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc);
            }
        }
    }

    copy_img(out_gpu, out);
    release(out_gpu.buf);
    release(arr_gpu.buf);
}
fn @iteration2i1m(body: fn(i32, i32, Acci16, Acci16, Acc) -> ()) = @|out: Img, img: Img, map: Img, bh_lower: BoundaryFni16, bh_upper: BoundaryFni16| {
    let coarsening_factor = 1;
    let acc   = accelerator(device_id);
    let grid  = (out.width, out.height / coarsening_factor, 1);
    let block = (32, 4, 1);

    let out_gpu = alloc_img[i16](out, acc.alloc);
    let img_gpu = alloc_img[i16](img, acc.alloc);
    let map_gpu = alloc_img[f32](map, acc.alloc);
    copy_img(img, img_gpu);
    copy_img(map, map_gpu);

    for benchmark_acc(acc) {
        for work_item in acc.exec(grid, block) {
            let bdim_y = work_item.bdimy();
            let gid_x  = work_item.tidx() + work_item.bdimx() * work_item.bidx();
            let gid_y  = work_item.tidy() + work_item.bdimy() * work_item.bidy() * coarsening_factor;
            let out_acc = get_acci16(out_gpu, set_pixeli16_fn(out_gpu), get_pixeli16_fn(out_gpu));
            let img_acc = get_acci16_bh(img_gpu, set_pixeli16_fn(img_gpu), get_pixeli16_fn(img_gpu), (Boundary::Unknown, Boundary::Unknown), bh_lower, bh_upper);
            let map_acc = get_acc(map_gpu, set_pixel_fn(map_gpu), get_pixel_fn(map_gpu));

            for i in unroll(0, coarsening_factor) {
                @body(gid_x, gid_y + i * bdim_y, out_acc, img_acc, map_acc);
            }
        }
    }

    copy_img(out_gpu, out);
    release(out_gpu.buf);
    release(img_gpu.buf);
    release(map_gpu.buf);
}
fn @iteration3i(body: fn(i32, i32, Acci16, Acci16, Acci16) -> ()) = @|out: Img, img: Img, tmp: Img| {
    let coarsening_factor = 1;
    let acc   = accelerator(device_id);
    let grid  = (out.width, out.height / coarsening_factor, 1);
    let block = (32, 4, 1);

    let out_gpu = alloc_img[i16](out, acc.alloc);
    let img_gpu = alloc_img[i16](img, acc.alloc);
    let tmp_gpu = alloc_img[i16](tmp, acc.alloc);
    copy_img(img, img_gpu);
    copy_img(tmp, tmp_gpu);

    for benchmark_acc(acc) {
        for work_item in acc.exec(grid, block) {
            let bdim_y = work_item.bdimy();
            let gid_x  = work_item.tidx() + work_item.bdimx() * work_item.bidx();
            let gid_y  = work_item.tidy() + work_item.bdimy() * work_item.bidy() * coarsening_factor;
            let out_acc = get_acci16(out_gpu, set_pixeli16_fn(out_gpu), get_pixeli16_fn(out_gpu));
            let img_acc = get_acci16(img_gpu, set_pixeli16_fn(img_gpu), get_pixeli16_fn(img_gpu));
            let tmp_acc = get_acci16(tmp_gpu, set_pixeli16_fn(tmp_gpu), get_pixeli16_fn(tmp_gpu));

            for i in unroll(0, coarsening_factor) {
                @body(gid_x, gid_y + i * bdim_y, out_acc, img_acc, tmp_acc);
            }
        }
    }

    copy_img(out_gpu, out);
    release(out_gpu.buf);
    release(img_gpu.buf);
    release(tmp_gpu.buf);
}
fn @reduce(body: fn(i32, i32, Acci16) -> i32) = @|img: Img| -> i32 {
    let acc = accelerator(device_id);
    let tmp_buf = acc.alloc(sizeof[i32]());
    let tmp     = bitcast[&mut addrspace(1)[i32], &[i8]](tmp_buf.data);
    let sum_buf = alloc_cpu(sizeof[i32]());
    bitcast[&mut[i32], &[i8]](sum_buf.data)(0) = 0;
    copy(sum_buf, tmp_buf);

    let img_gpu = alloc_img[i16](img, acc.alloc);
    copy_img(img, img_gpu);

    let grid  = (img_gpu.width, 1, 1);
    let block = (128, 1, 1);
    for benchmark_acc(acc) {
        for work_item in acc.exec(grid, block) {
            let tid_x  = work_item.tidx();
            let bid_x  = work_item.bidx();
            let bdim_x = work_item.bdimx();
            let gid_x  = tid_x + bdim_x * bid_x;
            let sm_sum = reserve_shared[i32](128);
            let mut sum = 0;
            let img_acc = get_acci16(img_gpu, set_pixeli16_fn(img_gpu), get_pixeli16_fn(img_gpu));
            for y in range(0, img_gpu.height) {
                sum += @body(gid_x, y, img_acc);
            }
            sm_sum(tid_x) = sum;

            for o in for_shift_down(128 >> 1, 0) {
                acc.barrier();
                if tid_x < o {
                    sm_sum(tid_x) += sm_sum(tid_x + o);
                }
            }

            acc.barrier();

            if tid_x == 0 {
                atomic_add_global(&mut tmp(0), sm_sum(0));
            }
        }
    }

    copy(tmp_buf, sum_buf);
    let sum = bitcast[&[i32], &[i8]](sum_buf.data)(0) / iter_acc;
    release(sum_buf);
    release(tmp_buf);
    release(img_gpu.buf);
    sum
}
fn @histogram(body: fn(i32, i32, Acc) -> i32) = @|img: Img| -> Buffer {
    let acc      = accelerator(device_id);
    let hist_buf = acc.alloc(256:i64 * sizeof[i32]());
    let     hist = bitcast[&mut addrspace(1)[i32], &[i8]](hist_buf.data);

    let img_gpu = alloc_img[f32](img, acc.alloc);
    copy_img(img, img_gpu);

    let grid  = (img.width, 1, 1);
    let block = (256, 1, 1);
    for benchmark_acc(acc) {
        for work_item in acc.exec((256, 1, 1), (64, 1, 1)) {
            hist(work_item.gidx()) = 0;
        }
        for work_item in acc.exec(grid, block) {
            let sm_hist = reserve_shared[i32](256);
            let img_acc = get_acc(img_gpu, set_pixel_fn(img_gpu), get_pixel_fn(img_gpu));
            for y in range(0, img.height) {
                let bin = @body(work_item.gidx(), y, img_acc);
                atomic_add_shared(&mut sm_hist(bin), 1);
            }
            atomic_add_global(&mut hist(work_item.tidx()), sm_hist(work_item.tidx()));
        }
    }

    hist_buf
}
fn @inclusive_scan(hist_buf: Buffer, size: i32) -> Buffer {
    let acc = accelerator(device_id);
    let scan_buf = acc.alloc(size as i64 * sizeof[i32]());
    let     scan = bitcast[&mut addrspace(1)[i32], &[i8]](scan_buf.data);
    let     hist = bitcast[&    addrspace(1)[i32], &[i8]](hist_buf.data);

    // perform scan per block in shared memory first
    let block_size = if size <= 256 { 128 } else { 256 };
    let num_blocks = size / (2 * block_size);
    let tmp_buf = acc.alloc(num_blocks as i64 * sizeof[i32]());
    let     tmp = bitcast[&mut addrspace(1)[i32], &[i8]](tmp_buf.data);

    let grid  = (size / 2, 1, 1);
    let block = (block_size, 1, 1);

    for benchmark_acc(acc) {
        for work_item in acc.exec(grid, block) {
            let tid_x = work_item.tidx();
            let bid_x = work_item.bidx();
            let gid_x = work_item.gidx();
            let sm_size = 2 * block_size;
            let sm_tmp = reserve_shared[i32](sm_size);

            // load input into shared memory
            sm_tmp(2 * tid_x)     = hist(2 * gid_x);
            sm_tmp(2 * tid_x + 1) = hist(2 * gid_x + 1);

            let mut offset = 1;
            for d in for_shift_down(sm_size >> 1, 0) { // build sum in place up the tree
                acc.barrier();
                if tid_x < d {
                    let ai = offset * (2 * tid_x + 1) - 1;
                    let bi = offset * (2 * tid_x + 2) - 1;
                    sm_tmp(bi) += sm_tmp(ai);
                }
                offset *= 2;
            }

            // clear the last element: exclusive scan only
            // if tid_x == 0 { sm_tmp(sm_size - 1) = 0; }

            // traverse down tree & build scan
            for d in for_shift_up(1, sm_size) {
                offset >>= 1;
                acc.barrier();
                if tid_x < d {
                    let ai = offset * (2 * tid_x + 1) - 1;
                    let bi = offset * (2 * tid_x + 2) - 1;
                    let tmp = sm_tmp(ai);
                    sm_tmp(ai) = sm_tmp(bi);
                    sm_tmp(bi) += tmp;
                }
            }

            acc.barrier();

            // write results to device memory
            if tid_x == 0 {
                tmp(bid_x) = sm_tmp(sm_size - 1);
            }
            scan(2 * gid_x)     = sm_tmp(2 * tid_x);
            scan(2 * gid_x + 1) = sm_tmp(2 * tid_x + 1);
        }
    }

    if num_blocks > 1 {
        // perform scan on block scan results
        let grid1  = (num_blocks, 1, 1);
        let block1 = (num_blocks, 1, 1);
        for benchmark_acc(acc) {
            for work_item in acc.exec(grid1, block1) {
                let tid_x = work_item.tidx();
                let sm_size = num_blocks;
                let sm_tmp = reserve_shared[i32](sm_size);

                // load input into shared memory
                sm_tmp(tid_x) = tmp(tid_x);

                acc.barrier();

                if tid_x == 0 {
                    let mut sum = sm_tmp(0);
                    for d in range(1, sm_size) {
                        sum += sm_tmp(d);
                        sm_tmp(d) = sum;
                    }
                }

                acc.barrier();

                // write results to device memory
                tmp(tid_x) = sm_tmp(tid_x);
            }
        }

        // add block scan results to array
        let grid2  = ((num_blocks-1) * block_size, 1, 1);
        let block2 = (block_size, 1, 1);
        for benchmark_acc(acc) {
            for work_item in acc.exec(grid2, block2) {
                let tid_x  = work_item.tidx();
                let bid_x  = work_item.bidx();
                let bdim_x = work_item.bdimx();
                let gid_x  = tid_x + bdim_x * bid_x + block_size; // first block holds already the correct scan results
                let sm_tmp = reserve_shared[i32](1);

                if tid_x == 0 {
                    sm_tmp(0) = tmp(bid_x);
                }

                acc.barrier();

                // write results to device memory
                scan(gid_x) += sm_tmp(0); // TODO: compensate for iter_acc
            }
        }
    }

    release(tmp_buf);
    scan_buf
}
fn @find_position(body: fn(i32) -> i32) = @|scan_buf: Buffer, size: i32| -> i32 {
    let acc      = accelerator(device_id);
    let size_buf = Buffer { data = bitcast[&mut[i8], &i32](&size), size = sizeof[i32](), device = 0 };
    let pos_buf  = acc.alloc(sizeof[i32]());
    let pos      = bitcast[&mut addrspace(1)[i32], &[i8]](pos_buf.data);
    let scan     = bitcast[&    addrspace(1)[i32], &[i8]](scan_buf.data);
    copy(size_buf, pos_buf);

    let grid  = (size, 1, 1);
    let block = (256, 1, 1);
    for benchmark_acc(acc) {
        for work_item in acc.exec(grid, block) {
            let gid_x = work_item.gidx();
            if @body(scan(gid_x)) != 0 {
                atomic_min_global(&mut pos(0), gid_x);
            }
        }
    }

    let mut result:i32;
    let r_buf = Buffer { data = bitcast[&mut [i8], &i32](&mut result), size = sizeof[i32](), device = 0 };
    copy(pos_buf, r_buf);
    release(pos_buf);
    result
}
